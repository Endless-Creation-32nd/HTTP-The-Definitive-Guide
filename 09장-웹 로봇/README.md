## 9장 웹 로봇

> **웹 로봇**은 사람과의 상호작용 없이 연속된 웹 트랜젝션들을 자동으로 수행하는 소프트웨어 프로그램

- 크롤러, 스파이더, 웜, 봇 등 다양한 이름으로 불린다.

웹 로봇의 몇 가지 예를 들어보자.

- 주식시장 서버에 매 분 HTTP GET 요청을 보내고, 얻은 데이터를 활용해 주가 추이 그래프를 생성하는 주식 그래프 로봇
- 월드 와이드 웹의 규모와 진화에 대한 통계 정보를 수집하는 웹 통계 조사 로봇
- 검색 데이터베이스를 만들기 위해 발견한 모든 문서를 수집하는 검색엔진 로봇
- 상품에 대한 가격 데이터베이스를 만들기 위해 온라인 쇼핑몰의 카탈로그에서 웹 페이지를 수집하는 가격비교 로봇

## 9.1 크롤러와 크롤링

> **웹 크롤러**  
> 웹 페이지 한 개 -> 한 개가 가리키는 모든 웹페이지 -> 모든 웹페이지가 가리키는 모든 웹페이지를 따라  
> 웹 링크를 재귀적으로 따라가는 로봇 (크롤러 혹은 스파이더)

### 9.1.1 어디에서 시작하는가: '루트 집합'

- 크롤러가 방문을 시작하는 URL들의 초기 집합은 **루트 집합(root set)** 이라고 불린다.
- 루트 집합을 고를 때, 모든 링크를 크롤링하면 결과적으로 관심 있는 웹 페이지들의 대부분을 가져올 수 있도록  
  충분히 다른 장소에서 URL들을 선택해야 한다.
- 일반적으로 좋은 루트 집합은 크고 인기있는 웹 사이트(www.google.com), 새로 생성된 페이지들의 목록,  
  자주 링크되지 않는 잘 알려지지 않은 페이지들의 목록으로 구성
- 대규모 크롤러 제품들은 사용자들이 루트 집합에 새 페이지나 잘 알려지지 않는 페이지들을 추가하는 기능을 제공한다.  
  시간이 지남에 따라 성장하며 새로운 크롤링을 위한 시드 목록이된다.

### 9.1.2 링크 추출과 상대 링크 정상화

- 크롤러는 웹을 돌아다니며 HTML 문서를 검색한다.
  - 검색한 각 페이지 안에 있는 URL 링크들을 파싱해서 크롤링할 페이지들의 목록에 추가한다.
- 크롤링을 진행하면서 탐색해야 할 새 링크를 발견하면, 이 목록은 급속히 확장된다.
- HTML을 파싱해서 이들 링크들을 추출하고 상대 링크를 절대 링크로 변환해야 한다.

### 9.1.3 순환 피하기

로봇이 웹을 크롤링할 때, **루프나 순환에 빠지지 않도록** 매우 조심해야한다.

```
A->B->C->A->B->C
```

- 로봇들은 순환을 피하기 위해 반드시 그들이 어디를 방문했는지 알아야 한다.
- 순환은 로봇을 함정에 빠뜨려서 멈추게 하거나 진행을 느려지게 한다.

### 9.1.4 루프와 중복

순한은 최소 다음의 세 가지 이유로 크롤러에게 해롭다.

1. 크롤러를 루프에 빠뜨린다. 루프에 빠져 어떤 페이지도 가져오지 못할 수 있다.
2. 같은 페이지를 계속 가져오면 웹 서버 부담이 된다.
3. 크롤러는 많은 수의 중복된 페이지들을 가져오게 된다.

### 9.1.5 빵 부스러기의 흔적

대규모 웹 크롤러가 그들이 방문한 곳을 관리하기 위해 사용하는 유용한 기법 몇 가지를 살펴보자.

- **트리와 해시 테이블**
  - 검색 트리나 해시 테이블을 사용
  - URL을 훨씬 빨리 찾아볼 수 있게 해주는 소프트웨어 자료구조다.
- **느슨한 존재 비트맵**

  - 존재 비트 배열(presence bit array)과 같은 느슨한 자료구조를 사용한다.
  - 각 URL은 해시 함수에 의해 고정된 크기의 숫자로 변환되고 배열 안에 대응하는 `'존재 비트(presence bit)'`를 갖는다.
  - URL이 크롤링 되었을 때, 해당하는 존재 비트가 생성
    - 이미 존재한다면, 크롤러는 그 URL이 이미 크롤링 되었다고 간주

- **체크포인트**
  - 로봇이 갑자기 중단될 경우를 대비해, 방문한 URL의 목록이 디스크에 저장되었는지 확인
- **파티셔닝**
  - 웹의 성장으로 인해 컴퓨터 한 대에서 크롤링을 완수하는 건 불가능해짐
  - 몇몇 대규모 웹 로봇은 각각이 분리된 한 대의 컴퓨인 로봇들이 동시에 일하고 있는 '농장'을 이용한다.
  - 각 로봇엔 URL들의 특정 '한 부분'이 할당되어 그에 대한 책임을 진다.

### 9.1.6 별칭(alias)과 로봇 순환

URL이 별칭을 가질 수 있는 이상 어떤 페이지를 이전에 방문했는지 말해주는게 쉽지 않을 수 있다.  
한 URL이 또 다른 URL에 대한 별칭이라면, 그 둘이 달라보여도 사실은 같은 리소스를 가리키고 있다.

다음은 다른 URL이 같은 리소스를 가리키게 되는 몇 가지 상황이다.

```
// 기본 포트가 80일 떄
http://www.foo.com/bar.html
http://www.foo.com:80/bar.html

// %7F이 ~와 같을 때
http://www.foo.com/~fred
http://www.foo.com/%7Ffred

// 태그에 따라 페이지가 바뀌지 않을 때
http://www.foo.com/x.html#early
http://www.foo.com/x.html#middle

// 서버가 대소문자 구분하지 않을 때
http://www.foo.com/readme.html
http://www.foo.com/README.HTML

// 기본 페이지가 index.html 일 때
http://www.foo.com/index.html
http://www.foo.com/

```

### 9.1.7 URL 정규화하기

URL들을 표준 형식으로 **'정규화'** 함으로써 **다른 URL과 같은 리소스를 가리키고 있음이 확실한 것들은  
미리 제거**하려 시도한다. 로봇은 다음과 같은 방식으로 모든 URL을 정규화된 형식으로 변환할 수 있다.

- 포트 번호가 명시되지 않았다면, 호스트 명에 ':80' 추가
- 모든 %xx 이스케이핑된 문자들을 대응되는 문자로 변환
- \# 태그들을 제거

### 9.1.8 파일 시스템 링크 순환

파일 시스템의 심벌릭 링크 순환은 서버 관리자가 실수로 만들게 되는게 보통이지만,  
때때로 누군가 로봇을 함정에 빠뜨리기 위해 악의적으로 만들기도 한다.

![](https://user-images.githubusercontent.com/39042837/108512587-c9584200-7304-11eb-8984-d084fcead901.jpeg)

(a)의 파일시스템을 사용해, 웹 크롤러는 다음의 동작을 수행한다.

- GET http://www.foo.com/index.html
  - /index.html을 가져와서, subdir/index.html로 이어지는 링크 발견
- GET http://www.foo.com/subdir/index.html
  - subdir/index.html을 가져와서, subdir/logo.gif로 이어지는 링크 발견
- GET http://www.foo.com/subdir/logo.gif
  - subdir/logo.gif를 가져오고, 더 이상 링크가 없으므로 완료

(b)의 파일시스템 에서는 다음과 같은 일이 벌어진다.

- GET http://www.foo.com/index.html
  - /index.html을 가져와서, subdir/index.html로 이어지는 링크 발견
- GET http://www.foo.com/subdir/index.html
  - subdir/index.html을 가져왔지만 같은 index.html로 돌아간다.
- GET http://www.foo.com/subdir/subdir/index.html
  - subdir/subdir/index.html을 가져온다.
- GET http://www.foo.com/subdir/subdir/subdir/index.html
  - subdir/subdir/subdir/index.html을 가져온다.

(b)의 문제는 로봇이 심벌릭 링크 순환을 알아차리지 못하고 URL이 달라서 루프로 빠져들 위험이 있다.

### 9.1.9 동적 가상 웹 공간

악의적인 누군가가 로봇들을 함정으로 빠뜨리기 위해 의도적으로 복잡한 크롤러 루프를 만드는 경우도 있다.  
또는, 나쁜 뜻이 없음에도 자신도 모르게 심벌릭 링크나 동적 콘텐츠를 통한 크롤러 함정을 만들 수 있다.

### 9.1.10 루프와 중복 피하기

모든 순환을 피하는 완벽한 방법은 없다.  
웹은 로봇이 문제를 일으킬 가능성으로 가득 차 있다. 이러한 웹에서 로봇이 더 올바르게 동작하기 위해  
사용하는 기법들은 다음과 같다.

- `URL 정규화`
  - URL을 표준 형태로 변환함으로써, 같은 리소스를 가리키는 중복된 URL을 일부 회피
- `너비 우선 크롤링`
  - 방문할 URL들을 웹 사이트들 전체에 걸쳐 너비 우선으로 스케줄링하면, 순환의 영향을 최소화 할 수 있음
  - 깊이 우선 방식으로 운용하여 웹 사이트 하나에 성급하게 뛰어들어 순환을 건드리는 경우, 영원히 다른 사이트로 빠져나올 수 없음
- `스로틀링`
  - 로봇이 웹 사이트에서 일정 시간 동안 가져올 수 있는 페이지의 숫자를 스로틀링을 이용해 제한
- `URL 크기 제한`
  - 로봇은 일정 길이(보통 1KB)를 넘는 URL의 크롤링은 거부할 수 있음
  - URL이 순환으로 인해 길어지면, 결국에는 길이 제한으로 중단될 수 있음
  - 주의할 점은 이 기법을 적용하면 가져오지 못하는 콘텐츠들도 틀림없이 있을 수 있음
- `URL/사이트 블랙리스트`
  - 로봇 순환을 만들어 내거나 함정인 것으로 알려진 사이트와 URL 목록을 만들어 관리하고 그들을 피함
- `패턴 발견`

  - 파일 시스템의 심벌릭 링크를 통한 순환과 그와 비슷한 오설정들은 일정 패턴을 따름
  - 따라서 몇몇 로봇은 몇 가지 다른 주기의 반복 패턴을 감지

- `콘텐츠 지문(fingerprint)`

  - 페이지의 콘텐츠에서 몇 바이트를 얻어내어 체크섬을 계산
  - 이전에 보았던 체크섬을 발견하면 그 페이지의 링크는 크롤링하지 않음
  - 어떤 웹 서버들은 동적으로 그때그때 페이지를 수정하므로, 로봇들은 때때로 웹페이지 콘텐츠에 임베딩된 링크와  
    같은 특정 부분들을 체크섬 계산에서 빠뜨림

- `사람의 모니터링`
  - 로봇은 결국 자신에게 적용된 어떤 기법으로 해결할 수 없는 문제에 봉착하게 됨
  - 따라서, 사람이 쉽게 로봇의 진행 상황을 모니터링해서 즉각 인지할 수 있게 반드시 진단과 로깅을 포함해야 함

## 9.2 로봇의 HTTP

### 9.2.1 요청 헤더 식별하기

로봇의 능력, 신원, 출신을 알려주는 기본적인 몇 가지 헤더를 사이트에게 보내주는 것이 좋다.  
로봇 개발자들이 구현을 하도록 권장되는 기본적인 신원 식별 헤더들은 다음과 같다.

- `User-Agent`
  - 서버에게 요청을 만든 로봇의 이름
- `From`
  - 로봇의 사용자/관리자의 이메일 주소 제공
- `Referer`
  - 현재의 요청 URL을 포함한 문서의 URL을 제공

### 9.2.2 가상 호스팅

요청에 Host 헤더를 포함하지 않으면 로봇이 어떤 URL에 대해 잘못된 콘텐츠를 찾게 만든다.  
이러한 이유로 HTTP/1.1은 Host 헤더를 사용할 것을 요구한다.

### 9.2.3 조건부 요청

로봇 중 몇몇은 시간이나 엔터티 태그를 비교해서 그들이 받아간 마지막 버전 이후에 업데이트 된 것이  
있는지 알아보는 조건부 HTTP 요청을 구현한다.

HTTP 캐시가 전에 받아온 리소스의 로컬 사본이 유효한지 검사하는 방법과 매우 비슷하다.

### 9.2.4 응답 다루기

대다수 로봇은 GET 메소드로 콘텐츠 요청을 하기 때문에, 응답 다루기라고 부를 만한 일은 거의 하지 않는다.  
그러나, 웹 탐색이나 서버와 상호작용을 더 잘 해보려는 로봇들은 HTTP 응답을 다룰 필요가 있다.

- `상태 코드`
  - 최소한 일반적인 상태 코드나 예상할 수 있는 상태 코드를 다룰 수 있어야 한다.
  - 모든 로봇은 200 OK나, 404 Not Found와 같은 상태 코드를 이해해야 한다.
- `엔터티`
  - HTTP 헤더에 임베딩된 정보를 따라 로봇들은 엔터티 자체의 정보를 찾을 수 있다.

### 9.2.5 User-Agent 타기팅

- 웹 관리자들은 많은 로봇이 그들의 사이트를 방문할 것을 명심하고, 그 로봇들로부터의 요청을 예상해야 한다.
- 사이트 관리자들은 로봇의 요청을 다루기 위한 전략을 세워야 한다.
  - 예) 다양한 클라이언트에 잘 대응하는 유연한 페이지를 개발

## 9.3 부적절하게 동작하는 로봇들

- `폭주하는 로봇`
  - 로봇이 논리적 에러를 갖고 있거나 순환에 빠졌다면 웹 서버에 부하를 줄 수 있다.
- `오래된 URL`
  - 오래된 URL의 콘텐츠가 많이 바뀌었다면 로봇들은 존재하지 않는 URL에 대한 요청을 많이 보낼 수 있다.
- `길고 잘못된 URL`
  - 순환 및 프로그래밍 오류로 로봇은 크고 의미 없는 URL을 요청할 수 있다.
    - 웹 서버의 처리 능력에 영향을 주고, 웹 서버 접근 로그를 어지럽게 채우고, 허술한 웹 서버라면 장애를 일으킬 수 있다.
- `호기심이 지나친 로봇`

  - 사적인 데이터에 대한 URL을 얻어 그 데이터를 검색엔진이나 기타 애플리케이션을 통해 쉽게 접근할 수 있도록 할 수 있다.

- `동적 게이트웨이 접근`
  - 로봇은 게이트웨이 애플리케이션의 콘텐츠에 대한 URL로 요청할 수도 있다.
    - 여기서 얻은 데이터는 아마도 특수 목적을 위한 것일테고 처리 비용이 많이 들 것이다.
